{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOvj20LSx4WYBVe6CNC4U3Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJBqBs54m2e9"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","X_train = torch.FloatTensor ([[1,2, 1,11. [2,1,3,21. [3, 1,3,4]. 14, 1.5,5]. [1,7,5,5]. [1,2,5,6]. [1,6,6,6], [1,7,7.711) \n","y_train = torch.Float Tensor ([[0,0, 1], [0,0, 1], [0,0, 11, [0, 1,01, [0, 1,01, [0, 1,0], [1,0,0], [1,0,0]1) \n","index_y = torch. argmax(ly_trail \n","W = torch. randn( [2, 11, requires_grad=True) \n","b = torch. randn([1], requires_grad-True) \n","optimizer = torch.opt im.SGD([W,b], Ir=0.01) \n","def mode I_LinearRegression (x) : \n","  return torch. matmul (x,W) +b \n","for step in range (2000) : prediction = model_LinearRegression (x_train) # prediction = H(x) in lecture node \n","cost = torch. mean( (prediction - y_train) ** 2) \n","optimizer .zero_grad() \n","cost.backward(\n","x_train = torch.FloatTensor([[1,2,1,1],[2,2],[3,3]])\n","y_train = torch.FloatTensor([[10],[20],[30]])\n","W = torch.randn([2,1],requires_grad=True)\n","b = torch.randn([1],requires_grad=True) \n","optimizer = torch.optim.SGD([W,b], lr=0.01)\n","\n","def model_LinearRegression(x): \n","  return torch.matmul(x,W)+b \n","  \n","for step in range(2000): \n","  prediction = model_LinearRegression(x_train) # prediction = H(x) in lecture node \n","  cost = torch.mean((prediction - y_train) ** 2)\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","# Here, we are done for the training. Then, see the result. \n","x_test = torch. FloatTensor([[4,4]]) # We should only have one test datapoint at a time\n","model_test = model_LinearRegression(x_test)\n","print(model_test)\n","print(model_test.detach())\n","print(model_test.detach().item())"]}]}